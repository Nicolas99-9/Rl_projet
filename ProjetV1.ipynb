{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import sklearn.pipeline\n",
    "import sklearn.preprocessing\n",
    "from matplotlib import pyplot as plt\n",
    "from copy import deepcopy\n",
    "%matplotlib qt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import the environnement\n",
    "from bicycle_env import BicycleEnvironment\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.kernel_approximation import RBFSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env = BicycleEnvironment()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#basic constants : \n",
    "model_name = \"sgd\"\n",
    "nb_actions = 9\n",
    "nb_states = 10 #for the begguging\n",
    "\n",
    "scaler = sklearn.preprocessing.StandardScaler()\n",
    "env.reset()\n",
    "scaler.partial_fit([env.sensors[:5]])\n",
    "featurizer = sklearn.pipeline.FeatureUnion([\n",
    "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100)),\n",
    "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100)),\n",
    "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100)),\n",
    "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100))\n",
    "        ])\n",
    "featurizer.fit(scaler.transform([env.sensors[:5]]))\n",
    "#scaling ?\n",
    "scaling = False\n",
    "new_features = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# basic function to approximate the state value : \n",
    "#one model for each action\n",
    "class Approximator():\n",
    "    def __init__(self,function_type):\n",
    "        self.models = []\n",
    "        self.alls  = [[0,0,0,0,0]]\n",
    "        for i in range(nb_actions):\n",
    "            model = function_type()\n",
    "            env.reset()\n",
    "            #only takes the angles\n",
    "            X_train  = env.sensors[:5]\n",
    "            model.partial_fit([self.space_discrete(X_train)],[[0]])\n",
    "            self.models.append(model)\n",
    "        self.alls = []\n",
    "    def space_discrete(self,X):\n",
    "        if scaling:\n",
    "            '''scaler.partial_fit([X])\n",
    "            scaled = scaler.transform([X])\n",
    "            self.alls.append(X)\n",
    "            featurizer.fit(scaler.transform(self.alls))'''\n",
    "            self.alls.append(X)\n",
    "            scaled = scaler.transform([X])\n",
    "            featurized = featurizer.transform(scaled)\n",
    "            return featurized[0]\n",
    "        else:\n",
    "            if new_features:\n",
    "                x_2 = np.power(X[2],2)\n",
    "                x_0 = np.power(X[0],2)\n",
    "                res = [1,X[2],X[3],x_2,np.power(X[3],2),X[2]*X[3],X[0],X[1],x_0,np.power(X[1],2),\n",
    "                      X[0]*X[1],X[2]*X[0],X[2]*x_0,x_2*X[0]]\n",
    "                return res\n",
    "            return X\n",
    "    def predict_values(self, s, a=None):\n",
    "        if a != None:\n",
    "            return self.models[a].predict(self.space_discrete(s))\n",
    "        return [self.models[a].predict([self.space_discrete(s)]) for a in range(len(self.models))]\n",
    "    def update(self, s, a, y):\n",
    "        a_int = act_to_int(a)\n",
    "        self.models[a_int].partial_fit([self.space_discrete(s)],[a_int])\n",
    "    def retrain_scaler(self):\n",
    "        scaler.fit(self.alls)\n",
    "        featurizer.fit(scaler.transform(self.alls))\n",
    "        \n",
    "f = ()\n",
    "if model_name == \"svm\":\n",
    "    f = type(SVR())\n",
    "elif model_name== \"sgd\":\n",
    "    f = type(SGDRegressor())\n",
    "a = Approximator(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#metric choice : \n",
    "metric_id = 1\n",
    "max_tilt = np.pi / 6.\n",
    "def angle_between(p1, p2):\n",
    "    ang1 = np.arctan2(*p1[::-1])\n",
    "    ang2 = np.arctan2(*p2[::-1])\n",
    "    tmp =  np.rad2deg((ang1 - ang2) % (2 * np.pi))\n",
    "    return np.deg2rad((tmp + 180 + 360) % 360 - 180)\n",
    "def get_reward(state):\n",
    "    if metric_id ==0:\n",
    "        if np.abs(env.getTilt()) > max_tilt:\n",
    "            return -1.0\n",
    "        return 0.0\n",
    "    elif metric_id ==1:\n",
    "        #chute du velo\n",
    "        if np.abs(env.getTilt()) > max_tilt:\n",
    "            return -1.0\n",
    "        #cible atteinte\n",
    "        elif state[6]>100:\n",
    "            return 0.001\n",
    "        else:\n",
    "            #rien de special\n",
    "            tmp = angle_between((state[5],state[6]),(0,100))\n",
    "            return (4-np.power(tmp,2))*0.00004\n",
    "            \n",
    "def int_to_act(number_action):\n",
    "    a1 = (number_action/3)\n",
    "    a2 = (number_action%3)\n",
    "    return (a1,a2)\n",
    "def act_to_int(act):\n",
    "    return act[0] *3 +act[1]\n",
    "def epsilon_greedy_policy(estimated_reward,epsilon):\n",
    "    if  np.random.random() > epsilon:\n",
    "        #exploitation\n",
    "        best = np.argmax(estimated_reward)\n",
    "        return int_to_act(best)\n",
    "    else:\n",
    "        #exploration :\n",
    "        return np.random.randint(3,size=2)\n",
    "def update_wheel_trajectories(ax2):\n",
    "    #code from the pybrain example\n",
    "    front_lines = ax2.plot(env.get_xfhist(), env.get_yfhist(), 'r')\n",
    "    back_lines = ax2.plot(env.get_xbhist(), env.get_ybhist(), 'b')\n",
    "    plt.axis('equal')\n",
    "def to_action(action):\n",
    "    bar_t = [-2.0,0,2.0]#[-1.0,0,1.0]\n",
    "    speed = [-0.02,0,0.02]\n",
    "    return [bar_t[action[0]],bar_t[action[1]]]\n",
    "def sarsa_lambda(env,approx,graphics = False,nb_episodes=10):\n",
    "    discount_factor = 1.0\n",
    "    rewards_cumules = []\n",
    "    if graphics:\n",
    "        env.saveWheelContactTrajectories(True)\n",
    "        plt.ion()\n",
    "        fif = plt.figure(figsize=(8, 4))\n",
    "        ax1 = plt.subplot(1, 2, 1)\n",
    "        ax2 = plt.subplot(1, 2, 2) \n",
    "    for _ in range(nb_episodes):\n",
    "        #reset the environnment\n",
    "        env.reset()\n",
    "        #initialize the traces:\n",
    "        traces  = np.zeros((nb_states,nb_actions))\n",
    "        #next action\n",
    "        a_next = np.random.randint(3,size=2)\n",
    "        #time to learning\n",
    "        t = 0\n",
    "        reward_cuml = 0\n",
    "        epsilon = 1.0\n",
    "        #current state\n",
    "        current_state = env.sensors[:5]\n",
    "        while True:\n",
    "            if env.time_step>0:\n",
    "                if a_next != None:\n",
    "                    action = a_next \n",
    "                #make the action\n",
    "                env.performAction(to_action(action))\n",
    "                reward = get_reward(env.sensors)\n",
    "                #estimation\n",
    "                next_values = approx.predict_values(env.sensors[:5])\n",
    "                #next action\n",
    "                a_next = epsilon_greedy_policy(next_values,epsilon)\n",
    "                estimated_value =  reward + discount_factor * next_values[act_to_int(a_next)]\n",
    "                approx.update(current_state, action, estimated_value)\n",
    "                reward_cuml += reward * np.power(discount_factor,t)\n",
    "                epsilon *= 0.99\n",
    "                current_state = env.sensors[:5]\n",
    "            if reward == -1.0 or reward==0.001:\n",
    "                #print(\"Distance parcourue jusqu'a chute  : \" , env.sensors[6] , t)\n",
    "                rewards_cumules.append(reward_cuml)\n",
    "                if scaling:\n",
    "                    approx.retrain_scaler()\n",
    "                break\n",
    "            t += 1\n",
    "        if graphics:\n",
    "            ax1.cla()\n",
    "            ax1.plot(rewards_cumules, '.--')\n",
    "            update_wheel_trajectories(ax2)\n",
    "            plt.pause(0.001)\n",
    "sarsa_lambda(env,Approximator(f),nb_episodes = 300, graphics=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
